import asyncio
import base64
import imghdr
import io
import json
import tomllib
import traceback
import time
from uuid import uuid4
from datetime import datetime 

import aiosqlite
from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage, AIMessage
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
from langgraph.graph import START, MessagesState, StateGraph
from pydantic import BaseModel, Field
from openai import AsyncOpenAI
import re

# å®šä¹‰å·¥å…·æ¨¡å‹
class GenerateImage(BaseModel):
    """Generate a image using AI. ç”¨AIç”Ÿæˆä¸€ä¸ªå›¾ç‰‡ã€‚"""
    prompt: str = Field(..., description="The prompt(or description) of image")

class InternetAccess(BaseModel):
    """Access the internet to search for real-time information and answer queries."""
    query: str = Field(..., description="The query keywords for internet search engine")

class AIChatTester:
    def __init__(self, config_path="plugins/all_in_one_config.toml", main_config_path="main_config.toml"):
        # åŠ è½½é…ç½®
        with open(config_path, "rb") as f:
            plugin_config = tomllib.load(f)

        with open(main_config_path, "rb") as f:
            main_config = tomllib.load(f)

        config = plugin_config["Ai"]
        openai_config = main_config["OpenAI"]

        # è¯»å– [Ai.MainModel] è®¾ç½®
        model_config = plugin_config["Ai"]["MainModel"]
        self.base_url = model_config["base-url"] if model_config["base-url"] else openai_config["base-url"]
        self.api_key = model_config["api-key"] if model_config["api-key"] else openai_config["api-key"]

        self.model_name = model_config["model-name"]
        self.temperature = model_config["temperature"]
        self.max_history_messages = model_config["max-history-messages"]
        self.model_kwargs = model_config["model_kwargs"]
        self.prompt = model_config["prompt"]

        # è®¾ç½®æ¨¡æ€æ€§
        modalities = []
        if model_config["text-output"]:
            modalities.append("text")
        if model_config["image-output"]:
            modalities.append("image")
        if model_config["voice-output"] == "Native":
            modalities.append("audio")
            if not self.model_kwargs.get("audio", None):
                self.model_kwargs["audio"] = {}
            self.model_kwargs["audio"]["format"] = "wav"

        self.model_kwargs["modalities"] = modalities

        # å›¾ç‰‡ç”Ÿæˆé…ç½®
        image_config = plugin_config["Ai"]["GenerateImage"]
        self.image_base_url = image_config["base-url"] if image_config["base-url"] else openai_config["base-url"]
        self.image_api_key = image_config["api-key"] if image_config["api-key"] else openai_config["api-key"]
        self.image_output_type = image_config["image-output-type"]
        self.image_model_name = image_config["model-name"]
        self.image_size = image_config["size"]
        self.image_additional_param = image_config["additional-param"]
        
        # äº’è”ç½‘è®¿é—®é…ç½®
        internet_config = plugin_config["Ai"]["InternetAccess"]
        self.internet_access_base_url = internet_config["base-url"] if internet_config["base-url"] else openai_config["base-url"]
        self.internet_access_api_key = internet_config["api-key"] if internet_config["api-key"] else openai_config["api-key"]
        self.internet_access_model_name = internet_config["model-name"]
        
        # è¯­éŸ³è½¬æ¢é…ç½®
        speech2text_config = plugin_config["Ai"]["SpeechToText"]
        self.speech2text_base_url = speech2text_config["base-url"] if speech2text_config["base-url"] else openai_config["base-url"]
        self.speech2text_api_key = speech2text_config["api-key"] if speech2text_config["api-key"] else openai_config["api-key"]
        self.speech2text_model_name = speech2text_config["model-name"]
        
        text2speech_config = plugin_config["Ai"]["TextToSpeech"]
        self.text2speech_base_url = text2speech_config["base-url"] if text2speech_config["base-url"] else openai_config["base-url"]
        self.text2speech_api_key = text2speech_config["api-key"] if text2speech_config["api-key"] else openai_config["api-key"]
        self.text2speech_model_name = text2speech_config["model-name"]
        self.text2speech_voice = text2speech_config["voice"]
        self.text2speech_speed = text2speech_config["speed"]
        self.text2speech_additional_param = text2speech_config["additional-param"]

        # åˆå§‹åŒ– langchain
        self.llm = ChatOpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            model=self.model_name,
            temperature=self.temperature,
            model_kwargs=self.model_kwargs
        )

        # å·¥å…·ç»‘å®š
        tools = []
        if "image" in modalities:
            tools.append(GenerateImage)
        if model_config["internet-access"]:
            tools.append(InternetAccess)
        
        if tools:
            self.llm = self.llm.bind_tools(tools)

        self.sqlite_conn = None
        self.sqlite_saver = None
        self.ai = None
        self.thread_id = None
        self.inited = False

    async def async_init(self):
        try:
            if self.sqlite_conn:
                await self.sqlite_conn.close()
            
            # ä½¿ç”¨å†…å­˜æ•°æ®åº“ç”¨äºæµ‹è¯•
            self.sqlite_conn = await aiosqlite.connect(":memory:")
            self.sqlite_saver = AsyncSqliteSaver(self.sqlite_conn)

            workflow = StateGraph(state_schema=MessagesState)
            workflow.add_edge(START, "model")
            workflow.add_node("model", self.call_model)

            self.ai = workflow.compile(checkpointer=self.sqlite_saver)
            self.thread_id = str(uuid4())
            self.inited = True
            print("AIæµ‹è¯•å™¨åˆå§‹åŒ–å®Œæ¯•")
        except Exception as e:
            print(f"åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    async def call_model(self, state: MessagesState):
        """å¤„ç†æ‰€æœ‰ç±»å‹çš„æ¶ˆæ¯"""
        start_time = time.time()
        messages = state["messages"]

        # é™åˆ¶å†å²æ¶ˆæ¯æ•°é‡
        if len(messages) > self.max_history_messages:
            # ä¿ç•™ç³»ç»Ÿæç¤º(ç¬¬ä¸€æ¡)å’Œæœ€è¿‘çš„æ¶ˆæ¯
            messages = [messages[0]] + messages[-self.max_history_messages + 1:]
            state["messages"] = messages  # æ›´æ–°çŠ¶æ€ä¸­çš„æ¶ˆæ¯åˆ—è¡¨

        try:
            response = await self.llm.ainvoke(messages)
            end_time = time.time()
            print(f"æ¨¡å‹è°ƒç”¨è€—æ—¶: {end_time - start_time:.2f}ç§’")
            return {"messages": response}
        except Exception as e:
            print(f"æ¨¡å‹è°ƒç”¨å‡ºé”™: {str(e)}")
            raise

    async def process_message(self, user_input, message_type="text", image_data=None, voice_data=None):
        """å¤„ç†æ¶ˆæ¯å¹¶è·å–AIå“åº”"""
        total_start_time = time.time()
        
        if not self.inited:
            await self.async_init()

        try:
            # é…ç½®çº¿ç¨‹ID
            configurable = {
                "configurable": {
                    "thread_id": self.thread_id,
                }
            }

            # æ ¹æ®æ¶ˆæ¯ç±»å‹æ„å»ºè¾“å…¥
            if message_type == "text":
                input_message = [SystemMessage(content=self.prompt)]
                input_message.append(HumanMessage(content=user_input))
            
            elif message_type == "image" and image_data:
                image_base64 = image_data
                image_format = self.get_img_format(image_base64)
                input_message = [SystemMessage(content=self.prompt)]
                input_message.append(HumanMessage(content=[
                    {"type": "image_url", "image_url": {"url": f"data:image/{image_format};base64,{image_base64}"}},
                    {"type": "text", "text": user_input or "è¯¦ç»†æè¿°å›¾ç‰‡å†…å®¹æœ‰ä»€ä¹ˆ"},
                ]))
            
            elif message_type == "voice" and voice_data:
                voice_start_time = time.time()
                input_message = [SystemMessage(content=self.prompt)]
                # ä½¿ç”¨æ¨¡å‹åŸç”Ÿæ”¯æŒè¯­éŸ³è¾“å…¥
                if "audio" in self.model_kwargs.get("modalities", []):
                    wav_base64 = self.byte_to_base64(voice_data)
                    input_message.append(HumanMessage(content=[
                        {"type": "input_audio", "input_audio": {"data": wav_base64, "format": "wav"}},
                    ]))
                else:
                    # è¯­éŸ³è½¬æ–‡æœ¬å¤„ç†
                    text_input = await self.get_text_from_voice(voice_data)
                    input_message.append(HumanMessage(content=text_input))
                voice_end_time = time.time()
                print(f"è¯­éŸ³å¤„ç†è€—æ—¶: {voice_end_time - voice_start_time:.2f}ç§’")
            else:
                raise ValueError(f"æœªæ”¯æŒçš„æ¶ˆæ¯ç±»å‹: {message_type}")

            # è¯·æ±‚AI API
            print(f"è¯·æ±‚AI API, thread id: {self.thread_id}")
            model_start_time = time.time()
            output = await self.ai.ainvoke({"messages": input_message}, configurable)
            model_end_time = time.time()
            print(f"AIä¸»æ¨¡å‹å“åº”è€—æ—¶: {model_end_time - model_start_time:.2f}ç§’")
            
            # å¤„ç†AIå“åº”
            old_output = output
            last_message = output["messages"][-1]
            
            # è·å–æ–‡æœ¬å“åº”
            if "audio" in last_message.additional_kwargs:
                text_response = last_message.additional_kwargs['audio']['transcript']
            else:
                text_response = last_message.content
            
            print(f"\nAIå›å¤: {text_response}")
            
            # æ£€æŸ¥å·¥å…·è°ƒç”¨
            tool_calls_time = 0
            if last_message.additional_kwargs.get("tool_calls"):
                tool_start_time = time.time()
                for tool_call in last_message.additional_kwargs["tool_calls"]:
                    if tool_call["function"]["name"] == "GenerateImage":
                        print("\nğŸ–¼ï¸æ­£åœ¨ç”Ÿæˆå›¾ç‰‡...")
                        image_start_time = time.time()
                        try:
                            prompt = json.loads(tool_call["function"]["arguments"])["prompt"]
                            b64_list, generate_image_result = await self.generate_image(prompt, old_output)
                            
                            tool_message_content = ""
                            if generate_image_result:
                                tool_message_content = generate_image_result
                                print(f"\nç”Ÿæˆå›¾ç‰‡ç»“æœ: {generate_image_result}")
                            else:
                                image_links = []
                                for i, img_url in enumerate(b64_list):
                                    image_links.append(f"å›¾ç‰‡{i+1}: {img_url[:30]}...{img_url[-10:]}")
                                tool_message_content = "å·²ç”Ÿæˆå¦‚ä¸‹å›¾ç‰‡ï¼š\n" + "\n".join(image_links)
                                print(f"\nå›¾ç‰‡å·²ç”Ÿæˆï¼Œå…±{len(b64_list)}å¼ ")
                            
                            image_end_time = time.time()
                            print(f"å›¾ç‰‡ç”Ÿæˆè€—æ—¶: {image_end_time - image_start_time:.2f}ç§’")
                            
                            # å¤„ç†å·¥å…·å›è°ƒ
                            callback_start_time = time.time()
                            tool_message = ToolMessage(
                                tool_call_id=tool_call["id"],
                                # tool_call_id="GenerateImage",
                                content=tool_message_content,
                                name="GenerateImage",
                                additional_kwargs={"name": "GenerateImage"}
                            )
                            output = await self.ai.ainvoke({"messages": [tool_message]}, configurable)
                            last_message = output["messages"][-1]
                            callback_end_time = time.time()
                            print(f"å›¾ç‰‡å¤„ç†å›è°ƒè€—æ—¶: {callback_end_time - callback_start_time:.2f}ç§’")
                            print(f"\nAIå¯¹å›¾ç‰‡çš„å›å¤: {last_message.content}")
                            
                        except Exception as e:
                            print(f"ç”Ÿæˆå›¾ç‰‡å¤±è´¥: {str(e)}")
                            traceback.print_exc()
                    
                    elif tool_call["function"]["name"] == "InternetAccess":
                        print("\nğŸ”æ­£åœ¨æœç´¢äº’è”ç½‘...")
                        search_start_time = time.time()
                        try:
                            prompt = json.loads(tool_call["function"]["arguments"])["query"]
                            search_result = await self.internet_access(old_output, prompt)
                            search_end_time = time.time()
                            print(f"äº’è”ç½‘æœç´¢è€—æ—¶: {search_end_time - search_start_time:.2f}ç§’")
                            print(f"\næœç´¢ç»“æœ: {search_result}")
                            
                            callback_start_time = time.time()
                            tool_message = ToolMessage(
                                tool_call_id=tool_call["id"],
                                content=search_result,
                                name="InternetAccess"
                            )
                            output = await self.ai.ainvoke({"messages": [tool_message]}, configurable)
                            last_message = output["messages"][-1]
                            callback_end_time = time.time()
                            print(f"æœç´¢ç»“æœå¤„ç†å›è°ƒè€—æ—¶: {callback_end_time - callback_start_time:.2f}ç§’")
                            print(f"\nAIå¯¹æœç´¢ç»“æœçš„å›å¤: {last_message.content}")
                            
                        except Exception as e:
                            print(f"äº’è”ç½‘è®¿é—®å¤±è´¥: {str(e)}")
                            traceback.print_exc()
                
                tool_end_time = time.time()
                tool_calls_time = tool_end_time - tool_start_time
                print(f"å·¥å…·è°ƒç”¨æ€»è€—æ—¶: {tool_calls_time:.2f}ç§’")
            
            total_end_time = time.time()
            total_time = total_end_time - total_start_time
            print(f"\næ€»è€—æ—¶: {total_time:.2f}ç§’")
            
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œè®¡ç®—å·¥å…·è°ƒç”¨å æ€»æ—¶é—´çš„ç™¾åˆ†æ¯”
            if tool_calls_time > 0:
                tool_percentage = (tool_calls_time / total_time) * 100
                print(f"å·¥å…·è°ƒç”¨å æ€»æ—¶é—´çš„ {tool_percentage:.1f}%")
            
            return output
            
        except Exception as e:
            total_end_time = time.time()
            print(f"å¤„ç†æ¶ˆæ¯å¤±è´¥: {str(e)}")
            print(f"æ€»è€—æ—¶: {total_end_time - total_start_time:.2f}ç§’")
            traceback.print_exc()
            return None

    async def generate_image(self, prompt: str, input_message: dict) -> tuple:
        """ç”Ÿæˆå›¾ç‰‡"""
        start_time = time.time()
        client = AsyncOpenAI(
            base_url=self.image_base_url,
            api_key=self.image_api_key
        )

        try:
            b64_list = []
            chat_completion_resp = None
            if self.image_output_type == "imageGenerate":
                resp = await client.images.generate(
                    model=self.image_model_name,
                    prompt=prompt,
                    size=self.image_size,
                    n=1,
                    extra_body=self.image_additional_param
                )
                
                for item in resp.data:
                    b64_list.append(item.url)

            elif self.image_output_type == "chatCompletion":
                openai_messages = []
                openai_messages.append({"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±ç»˜ç”»å¤§å¸ˆï¼Œæ²¡æœ‰è°æ¯”ä½ æ›´ä¸“ä¸šã€‚è§„åˆ™ï¼š1ã€æ ¹æ®ç”¨æˆ·çš„è¦æ±‚ï¼Œç”Ÿæˆå›¾ç‰‡æˆ–ä¿®å›¾ã€‚2ã€å¯¹äºä¿®å›¾éœ€è¦ä¿æŒåŸå›¾çš„é£æ ¼ä¸å˜ã€‚3ã€ä½ å¿…é¡»åœ¨æœ¬è½®å¯¹è¯ä¸­ç»™å‡ºå›¾ç‰‡,å³ä½¿ç”¨æˆ·æ²¡æœ‰æä¾›å…·ä½“çš„éœ€æ±‚ã€‚4ã€ç»ä¸å…è®¸ç¼–é€ ä¸å­˜åœ¨çš„å›¾ç‰‡ã€‚ä½ å¿…é¡»éµå®ˆè¿™äº›è§„åˆ™ï¼Œå¦åˆ™ä½ å°†å¤±å»ä½ çš„å·¥ä½œã€‚"})
                for msg in input_message["messages"]:
                    if isinstance(msg, HumanMessage):
                        openai_messages.append({"role": "user", "content": msg.content})
                    elif isinstance(msg, AIMessage) and msg.content:
                        msg.tool_calls = []
                        openai_messages.append({"role": "assistant", "content": msg.content})
                    # elif isinstance(msg, ToolMessage) and msg.content:
                    #     openai_messages.append({"role": "tool", "content": msg.content})

                resp = await client.chat.completions.create(
                    model=self.image_model_name,
                    messages=openai_messages,
                    stream=False,
                )
                chat_completion_resp = resp.choices[0].message.content
                # æå–æ‰€æœ‰å›¾ç‰‡URL
                IMAGE_URL_PATTERN = r'\[image\]\((.*?)\)'
                img_urls = re.findall(IMAGE_URL_PATTERN, chat_completion_resp)
                for img_url in img_urls:
                    b64_list.append(img_url)

            end_time = time.time()
            print(f"å›¾ç‰‡ç”ŸæˆAPIè¯·æ±‚è€—æ—¶: {end_time - start_time:.2f}ç§’")
            return b64_list, chat_completion_resp
            
        except Exception as e:
            end_time = time.time()
            print(f"ç”Ÿæˆå›¾ç‰‡é”™è¯¯: {str(e)}")
            print(f"å¤±è´¥è€—æ—¶: {end_time - start_time:.2f}ç§’")
            traceback.print_exc()
            raise
    
    async def internet_access(self, input_message: dict, query: str) -> str:
        """äº’è”ç½‘è®¿é—®"""
        start_time = time.time()
        client = AsyncOpenAI(
            base_url=self.internet_access_base_url,
            api_key=self.internet_access_api_key
        )
        try:
            # Convert langchain messages to OpenAI format
            openai_messages = []
            openai_messages.append({"role": "system", "content": "ç°åœ¨æ—¶é—´æ˜¯ï¼š" + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "ï¼Œä½ æ˜¯ä¸€ä¸ªç½‘ç»œå®æ—¶ä¿¡æ¯æœç´¢å·¥å…·ï¼Œä½ æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œä½¿ç”¨è”ç½‘æœç´¢èƒ½åŠ›æœç´¢ç›¸å…³ä¿¡æ¯ï¼Œå¹¶æ€»ç»“è¿”å›æœç´¢ç»“æœ,è¦æ±‚ç®€æ˜æ‰¼è¦ï¼Œæ’ç‰ˆæ•´é½ã€‚"})
            for msg in input_message["messages"]:
                if isinstance(msg, HumanMessage):
                    openai_messages.append({"role": "user", "content": msg.content})
                elif isinstance(msg, AIMessage) and msg.content:
                    openai_messages.append({"role": "assistant", "content": msg.content})
            
            # å°†æœ€åä¸€ä¸ªæ¶ˆæ¯æ›¿æ¢ä¸ºæŸ¥è¯¢å†…å®¹
            if openai_messages and openai_messages[-1]["role"] == "user":
                openai_messages[-1]["content"] = query
            else:
                openai_messages.append({"role": "user", "content": query})
            
            resp = await client.chat.completions.create(
                model=self.internet_access_model_name,
                messages=openai_messages,
                stream=False,
            )
            end_time = time.time()
            print(f"äº’è”ç½‘è®¿é—®APIè¯·æ±‚è€—æ—¶: {end_time - start_time:.2f}ç§’")
            return resp.choices[0].message.content
        except Exception as e:
            end_time = time.time()
            print(f"äº’è”ç½‘è®¿é—®é”™è¯¯: {str(e)}")
            print(f"å¤±è´¥è€—æ—¶: {end_time - start_time:.2f}ç§’")
            traceback.print_exc()
            raise

    async def get_text_from_voice(self, voice_data: bytes) -> str:
        """è¯­éŸ³è½¬æ–‡æœ¬"""
        start_time = time.time()
        tempfile = io.BytesIO(voice_data)
        tempfile.name = "audio.wav"
        client = AsyncOpenAI(
            base_url=self.speech2text_base_url,
            api_key=self.speech2text_api_key
        )
        try:
            resp = await client.audio.transcriptions.create(
                model=self.speech2text_model_name,
                file=tempfile
            )
            end_time = time.time()
            print(f"è¯­éŸ³è½¬æ–‡æœ¬APIè¯·æ±‚è€—æ—¶: {end_time - start_time:.2f}ç§’")
            return resp.text
        except Exception as e:
            end_time = time.time()
            print(f"è¯­éŸ³è½¬æ–‡æœ¬é”™è¯¯: {str(e)}")
            print(f"å¤±è´¥è€—æ—¶: {end_time - start_time:.2f}ç§’")
            traceback.print_exc()
            raise

    async def get_voice_from_text(self, text: str) -> bytes:
        """æ–‡æœ¬è½¬è¯­éŸ³"""
        start_time = time.time()
        client = AsyncOpenAI(
            base_url=self.text2speech_base_url,
            api_key=self.text2speech_api_key
        )
        try:
            resp = await client.audio.speech.create(
                model=self.text2speech_model_name,
                response_format="wav",
                voice=self.text2speech_voice,
                speed=float(self.text2speech_speed),
                input=text,
            )
            end_time = time.time()
            print(f"æ–‡æœ¬è½¬è¯­éŸ³APIè¯·æ±‚è€—æ—¶: {end_time - start_time:.2f}ç§’")
            return resp.content
        except Exception as e:
            end_time = time.time()
            print(f"æ–‡æœ¬è½¬è¯­éŸ³é”™è¯¯: {str(e)}")
            print(f"å¤±è´¥è€—æ—¶: {end_time - start_time:.2f}ç§’")
            traceback.print_exc()
            raise

    @staticmethod
    def get_img_format(img_base64: str) -> str:
        """è·å–å›¾ç‰‡æ ¼å¼"""
        if ',' in img_base64:
            img_base64 = img_base64.split(',')[1]
        return imghdr.what(io.BytesIO(base64.b64decode(img_base64)))

    @staticmethod
    def byte_to_base64(data: bytes) -> str:
        """å­—èŠ‚è½¬base64"""
        return base64.b64encode(data).decode('utf-8')
        
    async def reset_conversation(self):
        """é‡ç½®å¯¹è¯"""
        self.thread_id = str(uuid4())
        print(f"å¯¹è¯å·²é‡ç½®ï¼Œæ–°çº¿ç¨‹ID: {self.thread_id}")


async def main():
    # åˆ›å»ºAIæµ‹è¯•å™¨å®ä¾‹
    start_time = time.time()
    ai_tester = AIChatTester()
    await ai_tester.async_init()
    init_time = time.time() - start_time
    print(f"åˆå§‹åŒ–è€—æ—¶: {init_time:.2f}ç§’")
    
    print("æ¬¢è¿ä½¿ç”¨AIå¯¹è¯æµ‹è¯•å™¨!")
    print("è¾“å…¥ 'exit' é€€å‡º")
    print("è¾“å…¥ 'reset' é‡ç½®å¯¹è¯")
    print("è¾“å…¥ 'image:URL' æµ‹è¯•å›¾ç‰‡è¾“å…¥ (ä¾‹å¦‚ 'image:https://example.com/image.jpg')")
    
    while True:
        user_input = input("\nè¯·è¾“å…¥: ")
        
        if user_input.lower() == 'exit':
            break
        elif user_input.lower() == 'reset':
            await ai_tester.reset_conversation()
            continue
        elif user_input.lower().startswith('image:'):
            # è¿™é‡Œå¯ä»¥å®ç°å›¾ç‰‡è¾“å…¥çš„å¤„ç†
            # ä¸ºäº†ç®€åŒ–ï¼Œå›¾ç‰‡URLåé¢çš„éƒ¨åˆ†ä½œä¸ºæ–‡æœ¬è¾“å…¥
            image_url = user_input[6:].strip()
            print(f"å›¾ç‰‡è¾“å…¥æš‚æœªå®ç°ï¼Œå›¾ç‰‡URL: {image_url}")
            continue
            
        await ai_tester.process_message(user_input)
        
if __name__ == "__main__":
    asyncio.run(main()) 